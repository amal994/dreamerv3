from functools import partial as bind

import embodied
import numpy as np

from . import run_utils

'''
did_happen_validation
validates the correctness of single-step in-distribution rev WM predictions

Steps are as follows: 
  1. We make the trained agent go through a regular test run based on its
      learned policy - we cache the state and action information for every
      transition that takes place
  2. Reset the environment
  3. Starting from the final state, we go in reverse and do the following for 
     every state:
      3.1 feed the cached state, along with the cached action (that led to it) 
          to the rev WM. We ask the rev WM to predict what the previous state 
          must have been.
      3.2 save the image generated by the rev WM 
      Note: Latents for carry are only updated based on the true cached 
      observations, not the imagined ones. Code for this can be seen in 
      Agent::rev_step_with_consequences(...)
'''

def did_happen_validation(make_agent, make_env, make_logger, args):
  assert args.from_checkpoint

  def rev_step_with_consequences(obs, alt_action, dup_carry):
    formatted_action = [{'action': np.array([alt_action], dtype=np.int32)}]
    dup_carry, output_image =  agent.rev_step_with_consequences(obs, formatted_action, dup_carry)
    return dup_carry, output_image['image'][0]

  agent = make_agent()
  experiment_tracker = run_utils.ExperimentTracker(args)
  experiment_tracker.setup_tracking(make_logger)
  trajectory_cache = run_utils.TrajectoryCache(['image', 'reward', 'is_first', 'is_last', 'is_terminal', 'log_reward', 'action'])
  image_util = run_utils.ImageUtil(base_folder=args.test_images_folder, experiment_label='did_happen_validation')

  fns = [bind(make_env, i, needs_episode_reset = True) for i in range(args.num_envs)]
  driver = embodied.Driver(fns, args.driver_parallel)
  driver.on_step(lambda tran, _: experiment_tracker.step.increment())
  driver.on_step(lambda tran, _: experiment_tracker.policy_fps.step())
  driver.on_step(experiment_tracker.log_step)
  driver.on_step(trajectory_cache.cache_partial_obs)
 
  checkpoint = embodied.Checkpoint()
  checkpoint.agent = agent
  checkpoint.load(args.from_checkpoint, keys=['agent'])

  print('Start evaluation')
  policy = lambda *args: agent.policy(*args, mode='eval')
  driver.reset(agent.init_policy)
  # Make the agent go through a regular based actions selected by its own policy
  decoded_fwd_images = None
  while experiment_tracker.step < args.steps:
    fwd_images = driver(policy, steps=10)
    decoded_fwd_images = fwd_images if decoded_fwd_images is None else np.concatenate([decoded_fwd_images, fwd_images], axis=0)
    experiment_tracker.log_stats()

  for current_step in reversed(range(1, trajectory_cache.get_cache_count())):
    current_obs = trajectory_cache.get_obs_at(current_step)
    del current_obs['action']

    driver.dup_carry, output_image = rev_step_with_consequences(current_obs, trajectory_cache.get_action_at(current_step-1), driver.dup_carry)

    image_util.print_all_images(actual_image=trajectory_cache.get_image_at(current_step-1),
                        fwd_pred_image=decoded_fwd_images[current_step-1], 
                        rev_pred_images=output_image[np.newaxis, :], 
                        name_prefix=str(current_step-1))

  experiment_tracker.experiment_complete()
